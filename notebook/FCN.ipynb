{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FCN Network Notebook\n",
    "### Import all needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import scipy.misc, scipy.ndimage.interpolation\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers import Input, merge, concatenate, Conv2D, MaxPooling2D, Activation, UpSampling2D, Dropout, Conv2DTranspose, UpSampling2D, Lambda\n",
    "from keras.layers.normalization import BatchNormalization as bn\n",
    "from keras.layers.merge import add\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.merge import add\n",
    "import numpy as np\n",
    "from keras.regularizers import l2\n",
    "import cv2\n",
    "import glob\n",
    "import h5py\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BilinearUpSampling2D(Layer):\n",
    "    \"\"\"Upsampling2D with bilinear interpolation.\"\"\"\n",
    "\n",
    "    def __init__(self, target_shape=None, data_format=None, **kwargs):\n",
    "        if data_format is None:\n",
    "            data_format = K.image_data_format()\n",
    "        assert data_format in {\n",
    "            'channels_last', 'channels_first'}\n",
    "        self.data_format = data_format\n",
    "        self.input_spec = [InputSpec(ndim=4)]\n",
    "        self.target_shape = target_shape\n",
    "        if self.data_format == 'channels_first':\n",
    "            self.target_size = (target_shape[2], target_shape[3])\n",
    "        elif self.data_format == 'channels_last':\n",
    "            self.target_size = (target_shape[1], target_shape[2])\n",
    "        super(BilinearUpSampling2D, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.data_format == 'channels_last':\n",
    "            return (input_shape[0], self.target_size[0],\n",
    "                    self.target_size[1], input_shape[3])\n",
    "        else:\n",
    "            return (input_shape[0], input_shape[1],\n",
    "                    self.target_size[0], self.target_size[1])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return K1.resize_images(inputs, size=self.target_size,\n",
    "                                method='bilinear')\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'target_shape': self.target_shape,\n",
    "                'data_format': self.data_format}\n",
    "        base_config = super(BilinearUpSampling2D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    \n",
    "class CroppingLike2D(Layer):\n",
    "    def __init__(self, target_shape, offset=None, data_format=None,\n",
    "                 **kwargs):\n",
    "        \"\"\"Crop to target.\n",
    "        If only one `offset` is set, then all dimensions are offset by this amount.\n",
    "        \"\"\"\n",
    "        super(CroppingLike2D, self).__init__(**kwargs)\n",
    "        self.data_format = conv_utils.normalize_data_format(data_format)\n",
    "        self.target_shape = target_shape\n",
    "        if offset is None or offset == 'centered':\n",
    "            self.offset = 'centered'\n",
    "        elif isinstance(offset, int):\n",
    "            self.offset = (offset, offset)\n",
    "        elif hasattr(offset, '__len__'):\n",
    "            if len(offset) != 2:\n",
    "                raise ValueError('`offset` should have two elements. '\n",
    "                                 'Found: ' + str(offset))\n",
    "            self.offset = offset\n",
    "        self.input_spec = InputSpec(ndim=4)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.data_format == 'channels_first':\n",
    "            return (input_shape[0],\n",
    "                    input_shape[1],\n",
    "                    self.target_shape[2],\n",
    "                    self.target_shape[3])\n",
    "        else:\n",
    "            return (input_shape[0],\n",
    "                    self.target_shape[1],\n",
    "                    self.target_shape[2],\n",
    "                    input_shape[3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_shape = K.int_shape(inputs)\n",
    "        if self.data_format == 'channels_first':\n",
    "            input_height = input_shape[2]\n",
    "            input_width = input_shape[3]\n",
    "            target_height = self.target_shape[2]\n",
    "            target_width = self.target_shape[3]\n",
    "            if target_height > input_height or target_width > input_width:\n",
    "                raise ValueError('The Tensor to be cropped need to be smaller'\n",
    "                                 'or equal to the target Tensor.')\n",
    "\n",
    "            if self.offset == 'centered':\n",
    "                self.offset = [int((input_height - target_height) / 2),\n",
    "                               int((input_width - target_width) / 2)]\n",
    "\n",
    "            if self.offset[0] + target_height > input_height:\n",
    "                raise ValueError('Height index out of range: '\n",
    "                                 + str(self.offset[0] + target_height))\n",
    "            if self.offset[1] + target_width > input_width:\n",
    "                raise ValueError('Width index out of range:'\n",
    "                                 + str(self.offset[1] + target_width))\n",
    "\n",
    "            return inputs[:,\n",
    "                          :,\n",
    "                          self.offset[0]:self.offset[0] + target_height,\n",
    "                          self.offset[1]:self.offset[1] + target_width]\n",
    "        elif self.data_format == 'channels_last':\n",
    "            input_height = input_shape[1]\n",
    "            input_width = input_shape[2]\n",
    "            target_height = self.target_shape[1]\n",
    "            target_width = self.target_shape[2]\n",
    "            if target_height > input_height or target_width > input_width:\n",
    "                raise ValueError('The Tensor to be cropped need to be smaller'\n",
    "                                 'or equal to the target Tensor.')\n",
    "\n",
    "            if self.offset == 'centered':\n",
    "                self.offset = [int((input_height - target_height) / 2),\n",
    "                               int((input_width - target_width) / 2)]\n",
    "\n",
    "            if self.offset[0] + target_height > input_height:\n",
    "                raise ValueError('Height index out of range: '\n",
    "                                 + str(self.offset[0] + target_height))\n",
    "            if self.offset[1] + target_width > input_width:\n",
    "                raise ValueError('Width index out of range:'\n",
    "                                 + str(self.offset[1] + target_width))\n",
    "            output = inputs[:,\n",
    "                            self.offset[0]:self.offset[0] + target_height,\n",
    "                            self.offset[1]:self.offset[1] + target_width,\n",
    "                            :]\n",
    "            return output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'target_shape': self.target_shape,\n",
    "                  'offset': self.offset,\n",
    "                  'data_format': self.data_format}\n",
    "        base_config = super(CroppingLike2D, self).get_config()\n",
    "return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data extraction \n",
    "Here the Data is taken from the data set and comverted to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TRAINING_PATH =\"data/train\"\n",
    "TEST_FOLDER = \"data/test\"\n",
    "OUTPUT_FOLDER = \"output/\"\n",
    "\n",
    "for folder in sorted( os.listdir( TRAINING_PATH ) ):\n",
    "    xTrainFiles = random.sample( sorted(glob.glob ( os.path.join( TRAINING_PATH , folder+ \"/*.png\" ) )) , 10 )\n",
    "\n",
    "    Y_train_image = cv2.imread(os.path.join(TRAINING_PATH, folder + \"/mask.png\") , 0 )\n",
    "    Y_train_image = cv2.resize(Y_train_image, (512, 512))\n",
    "    Y_train_image = (Y_train_image == 2)\n",
    "    Y_train_image = Y_train_image.astype(int)\n",
    "\n",
    "    for myFile in xTrainFiles:\n",
    "        image = cv2.imread (myFile,1)\n",
    "        image=cv2.resize(image, (512, 512))\n",
    "        X_train_data.append (image)\n",
    "\n",
    "        Y_Train_data.append( Y_train_image )\n",
    "\n",
    "\n",
    "for folder in sorted(os.listdir(TEST_FOLDER)):\n",
    "    XTestFiles = sorted( glob.glob(os.path.join(TEST_FOLDER,  folder+ \"/frame0050.png\") ))\n",
    "\n",
    "    for myFile in XTestFiles :\n",
    "        names.append( myFile[ -78 :-14 ] )\n",
    "\n",
    "        image = cv2.imread(myFile, 1)\n",
    "        shapes.append( (image.shape[0], image.shape[1]) )\n",
    "\n",
    "        image = cv2.resize(image, (512, 512))\n",
    "        X_test_data.append(image)\n",
    "        \n",
    "\n",
    "X_train = np.array( X_train_data )\n",
    "Y_train = np.array( Y_Train_data )\n",
    "X_test = np.array(X_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if os.path.isfile( 'ds-project4.h5'): \n",
    "    model = load_model( 'ds-project4-fcn-b16ep124.h5' ,custom_objects=\n",
    "{ 'BilinearUpSampling2D':BilinearUpSampling2D,'dice_coef_loss':dice_coef_loss,'dice_coef':dice_coef})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted = model.predict(X_test)\n",
    "    for i in range(len(shapes)):\n",
    "        image = (predicted[i] * 255).astype(np.uint8)\n",
    "        image = cv2.resize(image, (shapes[i][1], shapes[i][0]), interpolation=cv2.INTER_AREA)\n",
    "        image = (image != 0).astype(int) * 2\n",
    "cv2.imwrite(os.path.join(OUTPUT_FOLDER, names[i] + '.png'), image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
